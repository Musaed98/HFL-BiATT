{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a95949-7334-41a8-bc8d-05d81b039155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23 15:01:57,114 - INFO - Starting federated learning attack testing...\n",
      "2026-01-23 15:01:57,120 - INFO - \n",
      "================================================================================\n",
      "2026-01-23 15:01:57,122 - INFO - EXPERIMENT: Attack=label_flip, Defense=fedavg, Clients=4, Attackers=1\n",
      "2026-01-23 15:01:57,124 - INFO - ================================================================================\n",
      "2026-01-23 15:01:57,128 - INFO - Using device: cpu\n",
      "2026-01-23 15:01:57,354 - INFO - Loaded 28480 samples for testing\n",
      "2026-01-23 15:01:57,414 - INFO - Client 0 (HONEST): 7121 samples, 24 fraud (0.3%)\n",
      "2026-01-23 15:01:57,416 - INFO - Client 1 (HONEST): 7120 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:01:57,420 - INFO - Client 2 (ATTACKER): 7120 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:01:57,425 - INFO - Client 3 (HONEST): 7119 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:01:57,427 - INFO - Created 4 clients with 1 attackers\n",
      "2026-01-23 15:02:00,361 - INFO - Client 2: Initialized with LABEL FLIP attack.\n",
      "2026-01-23 15:02:00,372 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:02:00,374 - INFO - Round 1/3\n",
      "2026-01-23 15:02:00,375 - INFO - ----------------------------------------\n",
      "2026-01-23 15:02:55,514 - INFO - Global Model Performance -> AUC: 0.9764, F1: 0.1730, Recall: 0.9570, Precision: 0.0951\n",
      "2026-01-23 15:02:55,972 - INFO - Fraud Detection -> Recall: 1.0000, Precision: 0.0951\n",
      "2026-01-23 15:02:55,972 - WARNING - ??  LABEL FLIP ATTACK DETECTED: Precision collapsed while recall remains high\n",
      "2026-01-23 15:02:55,974 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:02:55,975 - INFO - Round 2/3\n",
      "2026-01-23 15:02:55,975 - INFO - ----------------------------------------\n",
      "2026-01-23 15:03:50,088 - INFO - Global Model Performance -> AUC: 0.9579, F1: 0.5387, Recall: 0.9355, Precision: 0.3783\n",
      "2026-01-23 15:03:50,504 - INFO - Fraud Detection -> Recall: 1.0000, Precision: 0.3783\n",
      "2026-01-23 15:03:50,506 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:03:50,507 - INFO - Round 3/3\n",
      "2026-01-23 15:03:50,507 - INFO - ----------------------------------------\n",
      "2026-01-23 15:04:43,762 - INFO - Global Model Performance -> AUC: 0.9487, F1: 0.6374, Recall: 0.9355, Precision: 0.4833\n",
      "2026-01-23 15:04:44,197 - INFO - Fraud Detection -> Recall: 1.0000, Precision: 0.4833\n",
      "2026-01-23 15:04:44,199 - INFO - ? DEFENSE STATUS: No attack impact detected with fedavg\n",
      "2026-01-23 15:04:44,203 - INFO - \n",
      "================================================================================\n",
      "2026-01-23 15:04:44,204 - INFO - EXPERIMENT: Attack=label_flip, Defense=median, Clients=4, Attackers=1\n",
      "2026-01-23 15:04:44,205 - INFO - ================================================================================\n",
      "2026-01-23 15:04:44,206 - INFO - Using device: cpu\n",
      "2026-01-23 15:04:44,411 - INFO - Loaded 28480 samples for testing\n",
      "2026-01-23 15:04:44,465 - INFO - Client 0 (HONEST): 7121 samples, 24 fraud (0.3%)\n",
      "2026-01-23 15:04:44,469 - INFO - Client 1 (ATTACKER): 7120 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:04:44,471 - INFO - Client 2 (HONEST): 7120 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:04:44,473 - INFO - Client 3 (HONEST): 7119 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:04:44,474 - INFO - Created 4 clients with 1 attackers\n",
      "2026-01-23 15:04:44,504 - INFO - Client 1: Initialized with LABEL FLIP attack.\n",
      "2026-01-23 15:04:44,524 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:04:44,525 - INFO - Round 1/3\n",
      "2026-01-23 15:04:44,526 - INFO - ----------------------------------------\n",
      "2026-01-23 15:05:36,879 - INFO - Global Model Performance -> AUC: 0.9938, F1: 0.2184, Recall: 0.9677, Precision: 0.1231\n",
      "2026-01-23 15:05:37,302 - INFO - Fraud Detection -> Recall: 1.0000, Precision: 0.1231\n",
      "2026-01-23 15:05:37,303 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:05:37,305 - INFO - Round 2/3\n",
      "2026-01-23 15:05:37,306 - INFO - ----------------------------------------\n",
      "2026-01-23 15:06:29,391 - INFO - Global Model Performance -> AUC: 0.9568, F1: 0.5071, Recall: 0.9570, Precision: 0.3450\n",
      "2026-01-23 15:06:29,812 - INFO - Fraud Detection -> Recall: 1.0000, Precision: 0.3450\n",
      "2026-01-23 15:06:29,813 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:06:29,814 - INFO - Round 3/3\n",
      "2026-01-23 15:06:29,815 - INFO - ----------------------------------------\n",
      "2026-01-23 15:07:22,623 - INFO - Global Model Performance -> AUC: 0.9567, F1: 0.7964, Recall: 0.9462, Precision: 0.6875\n",
      "2026-01-23 15:07:23,202 - INFO - Fraud Detection -> Recall: 1.0000, Precision: 0.6875\n",
      "2026-01-23 15:07:23,203 - INFO - ? DEFENSE STATUS: No attack impact detected with median\n",
      "2026-01-23 15:07:23,206 - INFO - \n",
      "================================================================================\n",
      "2026-01-23 15:07:23,206 - INFO - EXPERIMENT: Attack=backdoor, Defense=fedavg, Clients=4, Attackers=1\n",
      "2026-01-23 15:07:23,207 - INFO - ================================================================================\n",
      "2026-01-23 15:07:23,208 - INFO - Using device: cpu\n",
      "2026-01-23 15:07:23,404 - INFO - Loaded 28480 samples for testing\n",
      "2026-01-23 15:07:23,453 - INFO - Client 0 (ATTACKER): 7121 samples, 24 fraud (0.3%)\n",
      "2026-01-23 15:07:23,455 - INFO - Client 1 (HONEST): 7120 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:07:23,458 - INFO - Client 2 (HONEST): 7120 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:07:23,460 - INFO - Client 3 (HONEST): 7119 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:07:23,461 - INFO - Created 4 clients with 1 attackers\n",
      "2026-01-23 15:07:23,500 - INFO - Client 0: Injected BACKDOOR attack (50 samples, col=0).\n",
      "2026-01-23 15:07:23,523 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:07:23,524 - INFO - Round 1/3\n",
      "2026-01-23 15:07:23,526 - INFO - ----------------------------------------\n",
      "2026-01-23 15:08:18,320 - INFO - Global Model Performance -> AUC: 0.9848, F1: 0.0104, Recall: 1.0000, Precision: 0.0052\n",
      "2026-01-23 15:08:18,818 - INFO - Backdoor Success Rate: 0.8325 (lower = better)\n",
      "2026-01-23 15:08:18,819 - WARNING - ??  HIGH BACKDOOR SUCCESS RATE DETECTED\n",
      "2026-01-23 15:08:18,820 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:08:18,821 - INFO - Round 2/3\n",
      "2026-01-23 15:08:18,822 - INFO - ----------------------------------------\n",
      "2026-01-23 15:09:13,708 - INFO - Global Model Performance -> AUC: 0.9622, F1: 0.5687, Recall: 0.9570, Precision: 0.4045\n",
      "2026-01-23 15:09:14,261 - INFO - Backdoor Success Rate: 0.0039 (lower = better)\n",
      "2026-01-23 15:09:14,262 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:09:14,263 - INFO - Round 3/3\n",
      "2026-01-23 15:09:14,264 - INFO - ----------------------------------------\n",
      "2026-01-23 15:10:07,703 - INFO - Global Model Performance -> AUC: 0.9661, F1: 0.8091, Recall: 0.9570, Precision: 0.7008\n",
      "2026-01-23 15:10:08,184 - INFO - Backdoor Success Rate: 0.0012 (lower = better)\n",
      "2026-01-23 15:10:08,185 - INFO - ? DEFENSE STATUS: No attack impact detected with fedavg\n",
      "2026-01-23 15:10:08,190 - INFO - \n",
      "================================================================================\n",
      "2026-01-23 15:10:08,191 - INFO - EXPERIMENT: Attack=backdoor, Defense=trimmed_mean, Clients=4, Attackers=1\n",
      "2026-01-23 15:10:08,192 - INFO - ================================================================================\n",
      "2026-01-23 15:10:08,193 - INFO - Using device: cpu\n",
      "2026-01-23 15:10:08,386 - INFO - Loaded 28480 samples for testing\n",
      "2026-01-23 15:10:08,444 - INFO - Client 0 (HONEST): 7121 samples, 24 fraud (0.3%)\n",
      "2026-01-23 15:10:08,446 - INFO - Client 1 (HONEST): 7120 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:10:08,450 - INFO - Client 2 (ATTACKER): 7120 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:10:08,453 - INFO - Client 3 (HONEST): 7119 samples, 23 fraud (0.3%)\n",
      "2026-01-23 15:10:08,455 - INFO - Created 4 clients with 1 attackers\n",
      "2026-01-23 15:10:08,497 - INFO - Client 2: Injected BACKDOOR attack (50 samples, col=0).\n",
      "2026-01-23 15:10:08,506 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:10:08,507 - INFO - Round 1/3\n",
      "2026-01-23 15:10:08,508 - INFO - ----------------------------------------\n",
      "2026-01-23 15:11:01,921 - INFO - Global Model Performance -> AUC: 0.9871, F1: 0.0066, Recall: 1.0000, Precision: 0.0033\n",
      "2026-01-23 15:11:02,468 - INFO - Backdoor Success Rate: 0.9994 (lower = better)\n",
      "2026-01-23 15:11:02,470 - WARNING - ??  HIGH BACKDOOR SUCCESS RATE DETECTED\n",
      "2026-01-23 15:11:02,473 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:11:02,473 - INFO - Round 2/3\n",
      "2026-01-23 15:11:02,474 - INFO - ----------------------------------------\n",
      "2026-01-23 15:11:56,986 - INFO - Global Model Performance -> AUC: 0.9624, F1: 0.4648, Recall: 0.9570, Precision: 0.3069\n",
      "2026-01-23 15:11:57,476 - INFO - Backdoor Success Rate: 0.0062 (lower = better)\n",
      "2026-01-23 15:11:57,478 - INFO - \n",
      "----------------------------------------\n",
      "2026-01-23 15:11:57,480 - INFO - Round 3/3\n",
      "2026-01-23 15:11:57,481 - INFO - ----------------------------------------\n",
      "2026-01-23 15:12:50,553 - INFO - Global Model Performance -> AUC: 0.9728, F1: 0.5779, Recall: 0.9570, Precision: 0.4140\n",
      "2026-01-23 15:12:51,023 - INFO - Backdoor Success Rate: 0.0042 (lower = better)\n",
      "2026-01-23 15:12:51,025 - INFO - ? DEFENSE STATUS: No attack impact detected with trimmed_mean\n",
      "2026-01-23 15:12:51,028 - INFO - \n",
      "? All experiments completed!\n",
      "2026-01-23 15:12:51,029 - INFO - Check 'federated_attack_test.log' for results showing how defenses perform against attacks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score\n",
    "import logging\n",
    "import copy\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# UNIVERSAL LOGGING FIX\n",
    "class SafeStreamHandler(logging.StreamHandler):\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            if sys.platform.startswith('win'):\n",
    "                msg = msg.encode('ascii', 'replace').decode('ascii')\n",
    "            stream = self.stream\n",
    "            stream.write(msg + self.terminator)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('federated_attack_test.log', encoding='utf-8'),\n",
    "        SafeStreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Model Definition\n",
    "# ----------------------------\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.layer_norm1(x + attn_output)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm2(x + ff_output)\n",
    "        return x\n",
    "\n",
    "class ImprovedRNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_classes, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.attention_layers = nn.ModuleList([AttentionModule(hidden_dim * 2, dropout=dropout) for _ in range(2)])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.feature_extractor(x).unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        for layer in self.attention_layers:\n",
    "            lstm_out = layer(lstm_out)\n",
    "        pooled = F.adaptive_max_pool1d(lstm_out.transpose(1, 2), 1).squeeze(-1)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "# ----------------------------\n",
    "# Aggregation Strategies\n",
    "# ----------------------------\n",
    "\n",
    "def fed_avg_aggregate(client_updates):\n",
    "    keys = client_updates[0].keys()\n",
    "    aggregated = {}\n",
    "    for key in keys:\n",
    "        stacked = torch.stack([u[key] for u in client_updates])\n",
    "        aggregated[key] = torch.mean(stacked, dim=0)\n",
    "    return aggregated\n",
    "\n",
    "def median_aggregate(client_updates):\n",
    "    keys = client_updates[0].keys()\n",
    "    aggregated = {}\n",
    "    for key in keys:\n",
    "        stacked = torch.stack([u[key] for u in client_updates])\n",
    "        aggregated[key] = torch.median(stacked, dim=0).values\n",
    "    return aggregated\n",
    "\n",
    "def trimmed_mean_aggregate(client_updates, trim_ratio=0.2):\n",
    "    \"\"\"Trimmed mean with safer trimming ratio for small client counts\"\"\"\n",
    "    keys = client_updates[0].keys()\n",
    "    aggregated = {}\n",
    "    for key in keys:\n",
    "        stacked = torch.stack([u[key] for u in client_updates])\n",
    "        n = stacked.shape[0]\n",
    "        to_trim = max(1, int(trim_ratio * n))  # Always trim at least 1\n",
    "        \n",
    "        if n <= 2 or to_trim >= n // 2:\n",
    "            aggregated[key] = torch.mean(stacked, dim=0)\n",
    "        else:\n",
    "            flat = stacked.view(n, -1)\n",
    "            sorted_vals, _ = torch.sort(flat, dim=0)\n",
    "            trimmed = sorted_vals[to_trim : n - to_trim]\n",
    "            mean_flat = torch.mean(trimmed, dim=0)\n",
    "            aggregated[key] = mean_flat.view(stacked.shape[1:])\n",
    "    return aggregated\n",
    "\n",
    "# ----------------------------\n",
    "# Federated Client (SCHEDULER REMOVED)\n",
    "# ----------------------------\n",
    "\n",
    "class FederatedClient:\n",
    "    def __init__(self, model, train_data, train_labels, device, client_id, attack_type=None, trigger_col=0, epochs=3):\n",
    "        self.client_id = client_id\n",
    "        self.model = copy.deepcopy(model).to(device)\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.device = device\n",
    "        self.attack_type = attack_type\n",
    "        self.trigger_col = trigger_col\n",
    "        self.is_attacker = attack_type is not None\n",
    "\n",
    "        # Apply data-level attacks ONCE during initialization\n",
    "        if attack_type == \"label_flip\":\n",
    "            self.original_labels = self.train_labels.clone()\n",
    "            self.train_labels = 1 - self.train_labels\n",
    "            logging.info(f\"Client {client_id}: Initialized with LABEL FLIP attack.\")\n",
    "        elif attack_type == \"backdoor\":\n",
    "            self.original_data = self.train_data.clone()\n",
    "            n_backdoor = min(50, len(self.train_labels) // 20)  # Fewer backdoor samples\n",
    "            if n_backdoor > 0:\n",
    "                idx = np.random.choice(len(self.train_data), size=n_backdoor, replace=False)\n",
    "                self.train_data[idx, self.trigger_col] = 3.0\n",
    "                self.train_labels[idx] = 1\n",
    "                logging.info(f\"Client {client_id}: Injected BACKDOOR attack ({n_backdoor} samples, col={trigger_col}).\")\n",
    "\n",
    "        # REMOVED SCHEDULER - using fixed learning rate for stability\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "        fraud_count = self.train_labels.sum().item()\n",
    "        non_fraud_count = len(self.train_labels) - fraud_count\n",
    "        pos_weight = torch.tensor([max(1.0, non_fraud_count / (fraud_count + 1e-8)) * 2.0])\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "\n",
    "    def train(self, epochs=3):\n",
    "        \"\"\"Train client model and return update (preserves client state across rounds)\"\"\"\n",
    "        if self.attack_type == \"free_rider\":\n",
    "            bad_state = copy.deepcopy(self.model.state_dict())\n",
    "            for k in bad_state:\n",
    "                if \"classifier\" in k:\n",
    "                    bad_state[k] = torch.zeros_like(bad_state[k])\n",
    "            return {k: v.cpu() for k, v in bad_state.items()}\n",
    "\n",
    "        self.model.train()\n",
    "        labels_np = self.train_labels.cpu().numpy().astype(int)\n",
    "        class_counts = np.bincount(labels_np, minlength=2)\n",
    "        weights = 1.0 / (class_counts[labels_np] + 1e-8)\n",
    "        sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "        \n",
    "        batch_size = min(32, len(self.train_data))\n",
    "        loader = DataLoader(\n",
    "            TensorDataset(self.train_data, self.train_labels),\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for data, labels in loader:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                out = self.model(data.float()).squeeze()\n",
    "                if out.dim() == 0:\n",
    "                    out = out.unsqueeze(0)\n",
    "                loss = self.criterion(out, labels.float())\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                # SCHEDULER STEP REMOVED\n",
    "\n",
    "        update = copy.deepcopy({k: v.cpu() for k, v in self.model.state_dict().items()})\n",
    "\n",
    "        # Apply update-level attacks\n",
    "        if self.attack_type == \"sign_flip\":\n",
    "            for k in update:\n",
    "                update[k] = -update[k]\n",
    "            logging.info(f\"Client {self.client_id}: Applied SIGN-FLIP attack on model update.\")\n",
    "        elif self.attack_type == \"boost\":\n",
    "            for k in update:\n",
    "                update[k] = update[k] * 100\n",
    "            logging.info(f\"Client {self.client_id}: Applied BOOST attack on model update.\")\n",
    "\n",
    "        return update\n",
    "\n",
    "# ----------------------------\n",
    "# Server\n",
    "# ----------------------------\n",
    "\n",
    "class FederatedServer:\n",
    "    def __init__(self, global_model, aggregation=\"fedavg\"):\n",
    "        self.global_model = global_model\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def aggregate(self, client_updates):\n",
    "        if self.aggregation == \"fedavg\":\n",
    "            return fed_avg_aggregate(client_updates)\n",
    "        elif self.aggregation == \"median\":\n",
    "            return median_aggregate(client_updates)\n",
    "        elif self.aggregation == \"trimmed_mean\":\n",
    "            return trimmed_mean_aggregate(client_updates)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation: {self.aggregation}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Data Handling\n",
    "# ----------------------------\n",
    "\n",
    "def load_and_preprocess_data(sample_fraction=0.1):\n",
    "    \"\"\"Load dataset with appropriate sampling for testing\"\"\"\n",
    "    if not os.path.exists('creditcard.csv'):\n",
    "        raise FileNotFoundError(\"Please place 'creditcard.csv' in this directory.\")\n",
    "    \n",
    "    df = pd.read_csv('creditcard.csv', nrows=int(284807 * sample_fraction))\n",
    "    logging.info(f\"Loaded {len(df)} samples for testing\")\n",
    "    \n",
    "    df['Time_log'] = np.log1p(df['Time'])\n",
    "    df['Amount_log'] = np.log1p(df['Amount'])\n",
    "    feature_cols = [col for col in df.columns if col not in ['Time', 'Amount', 'Class']]\n",
    "    X = df[feature_cols].values\n",
    "    y = df['Class'].values\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = np.clip(X, -5, 5)\n",
    "    X = np.nan_to_num(X)\n",
    "    return X.astype(np.float32), y.astype(np.int64)\n",
    "\n",
    "def create_non_iid_client_data(X, y, n_clients=4, n_attackers=1, attack_type=\"label_flip\"):\n",
    "    \"\"\"\n",
    "    Create realistic client data distribution with proper bounds checking\n",
    "    \"\"\"\n",
    "    fraud_idx = np.where(y == 1)[0]\n",
    "    non_fraud_idx = np.where(y == 0)[0]\n",
    "    np.random.shuffle(fraud_idx)\n",
    "    np.random.shuffle(non_fraud_idx)\n",
    "\n",
    "    # Split indices for each client\n",
    "    fraud_splits = np.array_split(fraud_idx, n_clients)\n",
    "    non_fraud_splits = np.array_split(non_fraud_idx, n_clients)\n",
    "    \n",
    "    clients = []\n",
    "    # Randomly select attacker clients\n",
    "    all_client_ids = list(range(n_clients))\n",
    "    np.random.shuffle(all_client_ids)\n",
    "    attacker_ids = set(all_client_ids[:n_attackers])\n",
    "    \n",
    "    for client_id in range(n_clients):\n",
    "        fraud_indices = fraud_splits[client_id]\n",
    "        non_fraud_indices = non_fraud_splits[client_id]\n",
    "        \n",
    "        # Skip if no samples\n",
    "        if len(fraud_indices) == 0 and len(non_fraud_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Combine indices\n",
    "        idx = np.concatenate([fraud_indices, non_fraud_indices])\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "        Xc = torch.from_numpy(X[idx])\n",
    "        yc = torch.from_numpy(y[idx])\n",
    "        \n",
    "        # Skip tiny clients\n",
    "        if len(yc) < 50:\n",
    "            continue\n",
    "            \n",
    "        # Determine if attacker\n",
    "        is_attacker = client_id in attacker_ids\n",
    "        attack_config = attack_type if is_attacker else None\n",
    "        \n",
    "        # Log client statistics\n",
    "        fraud_count = torch.sum(yc).item()\n",
    "        total_count = len(yc)\n",
    "        client_type = \"ATTACKER\" if is_attacker else \"HONEST\"\n",
    "        logging.info(f\"Client {client_id} ({client_type}): {total_count} samples, \"\n",
    "                    f\"{fraud_count} fraud ({fraud_count/total_count:.1%})\")\n",
    "        \n",
    "        clients.append({\n",
    "            'client_id': client_id,\n",
    "            'X': Xc,\n",
    "            'y': yc,\n",
    "            'attack_type': attack_config\n",
    "        })\n",
    "    \n",
    "    logging.info(f\"Created {len(clients)} clients with {n_attackers} attackers\")\n",
    "    return clients\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation Functions\n",
    "# ----------------------------\n",
    "\n",
    "def evaluate_model(model, X, y, device, batch_size=256):\n",
    "    model.eval()\n",
    "    dataset = DataLoader(TensorDataset(torch.from_numpy(X), torch.from_numpy(y)), \n",
    "                        batch_size=min(batch_size, len(X)))\n",
    "    preds, labels, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data, lab in dataset:\n",
    "            data = data.to(device).float()\n",
    "            out = model(data).squeeze()\n",
    "            if out.dim() == 0:\n",
    "                out = out.unsqueeze(0)\n",
    "            prob = torch.sigmoid(out).cpu().numpy()\n",
    "            pred = (prob > 0.5).astype(int)\n",
    "            \n",
    "            # Handle different dimensionalities\n",
    "            if isinstance(pred, np.ndarray):\n",
    "                preds.extend(pred.flatten().tolist())\n",
    "            else:\n",
    "                preds.append(int(pred))\n",
    "                \n",
    "            if isinstance(lab, torch.Tensor):\n",
    "                labels.extend(lab.numpy().flatten().tolist())\n",
    "            else:\n",
    "                labels.append(int(lab))\n",
    "                \n",
    "            if isinstance(prob, np.ndarray):\n",
    "                probs.extend(prob.flatten().tolist())\n",
    "            else:\n",
    "                probs.append(float(prob))\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    preds = np.array(preds)\n",
    "    probs = np.array(probs)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if len(labels) == 0 or len(np.unique(labels)) < 2:\n",
    "        logging.warning(\"Insufficient or single-class evaluation data\")\n",
    "        return 0.5, 0.0, 0.0, 0.0\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except:\n",
    "        auc = 0.5\n",
    "        \n",
    "    try:\n",
    "        f1 = f1_score(labels, preds)\n",
    "        recall = recall_score(labels, preds)\n",
    "        precision = precision_score(labels, preds)\n",
    "    except:\n",
    "        f1 = recall = precision = 0.0\n",
    "    \n",
    "    return auc, f1, recall, precision\n",
    "\n",
    "def test_backdoor_success(model, X_clean, y_clean, trigger_col, device):\n",
    "    X_trigger = X_clean.copy()\n",
    "    X_trigger[:, trigger_col] = 3.0\n",
    "    \n",
    "    # Only test on originally clean non-fraud samples\n",
    "    non_fraud_indices = np.where(y_clean == 0)[0]\n",
    "    if len(non_fraud_indices) < 10:\n",
    "        return 0.0\n",
    "        \n",
    "    X_trigger_clean = X_trigger[non_fraud_indices]\n",
    "    y_clean_clean = y_clean[non_fraud_indices]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_size = min(256, len(X_trigger_clean))\n",
    "        probs_all = []\n",
    "        for i in range(0, len(X_trigger_clean), batch_size):\n",
    "            batch = X_trigger_clean[i:i+batch_size]\n",
    "            out = model(torch.from_numpy(batch).to(device).float()).squeeze()\n",
    "            if out.dim() == 0:\n",
    "                out = out.unsqueeze(0)\n",
    "            probs = torch.sigmoid(out).cpu().numpy()\n",
    "            probs_all.extend(probs.flatten().tolist() if isinstance(probs, np.ndarray) else [probs])\n",
    "    \n",
    "    if not probs_all:\n",
    "        return 0.0\n",
    "        \n",
    "    success_rate = np.mean(np.array(probs_all) > 0.5)\n",
    "    return success_rate\n",
    "\n",
    "def evaluate_fraud_recall(model, X, y, device, batch_size=256):\n",
    "    \"\"\"Measure recall specifically for FRAUD samples\"\"\"\n",
    "    model.eval()\n",
    "    dataset = DataLoader(TensorDataset(torch.from_numpy(X), torch.from_numpy(y)), \n",
    "                        batch_size=min(batch_size, len(X)))\n",
    "    fraud_preds = []\n",
    "    fraud_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataset:\n",
    "            # Get only fraud samples\n",
    "            fraud_mask = (labels == 1)\n",
    "            n_fraud = fraud_mask.sum().item()\n",
    "            if n_fraud == 0:\n",
    "                continue\n",
    "                \n",
    "            data_fraud = data[fraud_mask].to(device).float()\n",
    "            labels_fraud = labels[fraud_mask].numpy()\n",
    "            \n",
    "            out = model(data_fraud).squeeze()\n",
    "            if out.dim() == 0:\n",
    "                out = out.unsqueeze(0)\n",
    "            \n",
    "            prob = torch.sigmoid(out).cpu().numpy()\n",
    "            preds = (prob > 0.5).astype(int)\n",
    "            \n",
    "            # Convert to list format\n",
    "            if isinstance(preds, np.ndarray):\n",
    "                fraud_preds.extend(preds.flatten().tolist())\n",
    "            else:\n",
    "                fraud_preds.append(int(preds))\n",
    "                \n",
    "            if isinstance(labels_fraud, np.ndarray):\n",
    "                fraud_labels.extend(labels_fraud.flatten().tolist())\n",
    "            else:\n",
    "                fraud_labels.append(int(labels_fraud))\n",
    "    \n",
    "    if len(fraud_preds) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    fraud_preds = np.array(fraud_preds)\n",
    "    fraud_labels = np.array(fraud_labels)\n",
    "    \n",
    "    if len(fraud_labels) == 0 or len(np.unique(fraud_labels)) < 2:\n",
    "        return 1.0 if len(fraud_preds) > 0 and fraud_preds[0] == 1 else 0.0\n",
    "        \n",
    "    try:\n",
    "        return recall_score(fraud_labels, fraud_preds)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# ----------------------------\n",
    "# Main Experiment Runner\n",
    "# ----------------------------\n",
    "\n",
    "def run_experiment(attack_type, aggregation_method, rounds=3, n_clients=4, n_attackers=1):\n",
    "    logging.info(f\"\\n{'='*80}\")\n",
    "    logging.info(f\"EXPERIMENT: Attack={attack_type}, Defense={aggregation_method}, \"\n",
    "                f\"Clients={n_clients}, Attackers={n_attackers}\")\n",
    "    logging.info(f\"{'='*80}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    X, y = load_and_preprocess_data(sample_fraction=0.1)\n",
    "\n",
    "    # Create client data\n",
    "    client_configs = create_non_iid_client_data(X, y, n_clients=n_clients, \n",
    "                                              n_attackers=n_attackers, attack_type=attack_type)\n",
    "\n",
    "    # Safety check\n",
    "    if len(client_configs) < 2:\n",
    "        logging.error(f\"Insufficient clients created: {len(client_configs)}. Skipping experiment.\")\n",
    "        return\n",
    "\n",
    "    # Initialize clients\n",
    "    clients = []\n",
    "    input_size = X.shape[1]\n",
    "    global_model = ImprovedRNNLSTM(input_size=input_size, hidden_dim=64, num_classes=1).to(device)\n",
    "    \n",
    "    for config in client_configs:\n",
    "        client = FederatedClient(\n",
    "            global_model, \n",
    "            config['X'], \n",
    "            config['y'], \n",
    "            device, \n",
    "            client_id=config['client_id'],\n",
    "            attack_type=config['attack_type'],\n",
    "            trigger_col=0,\n",
    "            epochs=3\n",
    "        )\n",
    "        clients.append(client)\n",
    "    \n",
    "    server = FederatedServer(global_model, aggregation=aggregation_method)\n",
    "\n",
    "    # Main FL loop\n",
    "    for rnd in range(1, rounds + 1):\n",
    "        logging.info(f\"\\n{'-'*40}\")\n",
    "        logging.info(f\"Round {rnd}/{rounds}\")\n",
    "        logging.info(f\"{'-'*40}\")\n",
    "        \n",
    "        client_updates = []\n",
    "        \n",
    "        # Each client trains\n",
    "        for client in clients:\n",
    "            update = client.train(epochs=3)\n",
    "            client_updates.append(update)\n",
    "        \n",
    "        # Server aggregates updates\n",
    "        agg_state = server.aggregate(client_updates)\n",
    "        global_model.load_state_dict(agg_state)\n",
    "\n",
    "        # Evaluate global model\n",
    "        auc, f1, rec, prec = evaluate_model(global_model, X, y, device)\n",
    "        logging.info(f\"Global Model Performance -> AUC: {auc:.4f}, F1: {f1:.4f}, \"\n",
    "                    f\"Recall: {rec:.4f}, Precision: {prec:.4f}\")\n",
    "\n",
    "        # Attack detection\n",
    "        attack_detected = False\n",
    "        \n",
    "        if attack_type == \"backdoor\":\n",
    "            backdoor_rate = test_backdoor_success(global_model, X, y, trigger_col=0, device=device)\n",
    "            logging.info(f\"Backdoor Success Rate: {backdoor_rate:.4f} (lower = better)\")\n",
    "            if backdoor_rate > 0.3:\n",
    "                logging.warning(\"⚠️  HIGH BACKDOOR SUCCESS RATE DETECTED\")\n",
    "                attack_detected = True\n",
    "        \n",
    "        elif attack_type == \"label_flip\":\n",
    "            fraud_recall = evaluate_fraud_recall(global_model, X, y, device)\n",
    "            logging.info(f\"Fraud Detection -> Recall: {fraud_recall:.4f}, Precision: {prec:.4f}\")\n",
    "            if prec < 0.1 and fraud_recall > 0.9:\n",
    "                logging.warning(\"⚠️  LABEL FLIP ATTACK DETECTED: Precision collapsed while recall remains high\")\n",
    "                attack_detected = True\n",
    "        \n",
    "        # Final round summary\n",
    "        if rnd == rounds:\n",
    "            if attack_detected:\n",
    "                logging.info(f\"✅ DEFENSE STATUS: Defense {aggregation_method} detected attack\")\n",
    "            else:\n",
    "                logging.info(f\"✅ DEFENSE STATUS: No attack impact detected with {aggregation_method}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Run Experiments\n",
    "# ----------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Starting federated learning attack testing...\")\n",
    "    \n",
    "    experiments = [\n",
    "        {\"attack\": \"label_flip\", \"defense\": \"fedavg\"},\n",
    "        {\"attack\": \"label_flip\", \"defense\": \"median\"},\n",
    "        {\"attack\": \"backdoor\", \"defense\": \"fedavg\"},\n",
    "        {\"attack\": \"backdoor\", \"defense\": \"trimmed_mean\"}\n",
    "    ]\n",
    "    \n",
    "    for exp in experiments:\n",
    "        run_experiment(\n",
    "            attack_type=exp[\"attack\"],\n",
    "            aggregation_method=exp[\"defense\"],\n",
    "            rounds=3,\n",
    "            n_clients=4,\n",
    "            n_attackers=1\n",
    "        )\n",
    "    \n",
    "    logging.info(\"\\n✅ All experiments completed!\")\n",
    "    logging.info(\"Check 'federated_attack_test.log' for results showing how defenses perform against attacks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cbdb90-6fc8-4061-81da-79c13cf8087b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
