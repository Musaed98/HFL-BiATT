{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 21:52:12,044 - INFO - NumExpr defaulting to 8 threads.\n",
      "2025-02-18 21:52:12,263 - INFO - Using device: cpu\n",
      "2025-02-18 21:52:12,296 - INFO - \n",
      "Training base_model\n",
      "2025-02-18 21:52:29,083 - INFO - base_model - Epoch 1/15, Loss: 0.9093, Accuracy: 0.7525\n",
      "2025-02-18 21:52:46,077 - INFO - base_model - Epoch 2/15, Loss: 0.6583, Accuracy: 0.8577\n",
      "2025-02-18 21:53:03,672 - INFO - base_model - Epoch 3/15, Loss: 0.5764, Accuracy: 0.8687\n",
      "2025-02-18 21:53:21,598 - INFO - base_model - Epoch 4/15, Loss: 0.5403, Accuracy: 0.8808\n",
      "2025-02-18 21:53:39,222 - INFO - base_model - Epoch 5/15, Loss: 0.5212, Accuracy: 0.8873\n",
      "2025-02-18 21:53:55,980 - INFO - base_model - Epoch 6/15, Loss: 0.5075, Accuracy: 0.8916\n",
      "2025-02-18 21:54:12,808 - INFO - base_model - Epoch 7/15, Loss: 0.4993, Accuracy: 0.8968\n",
      "2025-02-18 21:54:29,557 - INFO - base_model - Epoch 8/15, Loss: 0.4945, Accuracy: 0.8966\n",
      "2025-02-18 21:54:46,181 - INFO - base_model - Epoch 9/15, Loss: 0.4898, Accuracy: 0.9009\n",
      "2025-02-18 21:55:02,782 - INFO - base_model - Epoch 10/15, Loss: 0.4874, Accuracy: 0.9006\n",
      "2025-02-18 21:55:19,271 - INFO - base_model - Epoch 11/15, Loss: 0.4838, Accuracy: 0.9001\n",
      "2025-02-18 21:55:36,015 - INFO - base_model - Epoch 12/15, Loss: 0.4824, Accuracy: 0.9020\n",
      "2025-02-18 21:55:52,689 - INFO - base_model - Epoch 13/15, Loss: 0.4807, Accuracy: 0.9027\n",
      "2025-02-18 21:56:09,164 - INFO - base_model - Epoch 14/15, Loss: 0.4798, Accuracy: 0.9034\n",
      "2025-02-18 21:56:25,819 - INFO - base_model - Epoch 15/15, Loss: 0.4799, Accuracy: 0.9033\n",
      "2025-02-18 21:56:25,820 - INFO - \n",
      "Training with_attention\n",
      "2025-02-18 21:56:48,392 - INFO - with_attention - Epoch 1/15, Loss: 0.7289, Accuracy: 0.8471\n",
      "2025-02-18 21:57:10,836 - INFO - with_attention - Epoch 2/15, Loss: 0.6136, Accuracy: 0.8663\n",
      "2025-02-18 21:57:33,410 - INFO - with_attention - Epoch 3/15, Loss: 0.5617, Accuracy: 0.8798\n",
      "2025-02-18 21:57:55,902 - INFO - with_attention - Epoch 4/15, Loss: 0.5282, Accuracy: 0.8856\n",
      "2025-02-18 21:58:19,967 - INFO - with_attention - Epoch 5/15, Loss: 0.5101, Accuracy: 0.8932\n",
      "2025-02-18 21:58:43,237 - INFO - with_attention - Epoch 6/15, Loss: 0.5013, Accuracy: 0.8955\n",
      "2025-02-18 21:59:06,444 - INFO - with_attention - Epoch 7/15, Loss: 0.4955, Accuracy: 0.8985\n",
      "2025-02-18 21:59:29,369 - INFO - with_attention - Epoch 8/15, Loss: 0.4902, Accuracy: 0.8990\n",
      "2025-02-18 21:59:51,981 - INFO - with_attention - Epoch 9/15, Loss: 0.4867, Accuracy: 0.9011\n",
      "2025-02-18 22:00:14,960 - INFO - with_attention - Epoch 10/15, Loss: 0.4831, Accuracy: 0.9015\n",
      "2025-02-18 22:00:37,688 - INFO - with_attention - Epoch 11/15, Loss: 0.4805, Accuracy: 0.9034\n",
      "2025-02-18 22:01:00,396 - INFO - with_attention - Epoch 12/15, Loss: 0.4774, Accuracy: 0.9056\n",
      "2025-02-18 22:01:22,846 - INFO - with_attention - Epoch 13/15, Loss: 0.4762, Accuracy: 0.9056\n",
      "2025-02-18 22:01:45,718 - INFO - with_attention - Epoch 14/15, Loss: 0.4746, Accuracy: 0.9065\n",
      "2025-02-18 22:02:11,226 - INFO - with_attention - Epoch 15/15, Loss: 0.4731, Accuracy: 0.9070\n",
      "2025-02-18 22:02:11,228 - INFO - \n",
      "Training with_bidirectional\n",
      "2025-02-18 22:03:19,266 - INFO - with_bidirectional - Epoch 1/15, Loss: 0.6953, Accuracy: 0.8567\n",
      "2025-02-18 22:04:24,548 - INFO - with_bidirectional - Epoch 2/15, Loss: 0.5903, Accuracy: 0.8722\n",
      "2025-02-18 22:05:34,237 - INFO - with_bidirectional - Epoch 3/15, Loss: 0.5548, Accuracy: 0.8823\n",
      "2025-02-18 22:06:40,555 - INFO - with_bidirectional - Epoch 4/15, Loss: 0.5328, Accuracy: 0.8821\n",
      "2025-02-18 22:07:46,766 - INFO - with_bidirectional - Epoch 5/15, Loss: 0.5195, Accuracy: 0.8927\n",
      "2025-02-18 22:13:28,487 - INFO - with_bidirectional - Epoch 6/15, Loss: 0.5102, Accuracy: 0.8907\n",
      "2025-02-18 22:14:27,997 - INFO - with_bidirectional - Epoch 7/15, Loss: 0.5018, Accuracy: 0.8994\n",
      "2025-02-18 22:15:30,333 - INFO - with_bidirectional - Epoch 8/15, Loss: 0.4948, Accuracy: 0.8993\n",
      "2025-02-18 22:16:50,176 - INFO - with_bidirectional - Epoch 9/15, Loss: 0.4899, Accuracy: 0.9031\n",
      "2025-02-18 22:17:56,757 - INFO - with_bidirectional - Epoch 10/15, Loss: 0.4849, Accuracy: 0.9036\n",
      "2025-02-18 22:19:03,536 - INFO - with_bidirectional - Epoch 11/15, Loss: 0.4811, Accuracy: 0.9052\n",
      "2025-02-18 22:20:15,526 - INFO - with_bidirectional - Epoch 12/15, Loss: 0.4758, Accuracy: 0.9047\n",
      "2025-02-18 22:21:23,128 - INFO - with_bidirectional - Epoch 13/15, Loss: 0.4744, Accuracy: 0.9072\n",
      "2025-02-18 22:22:30,274 - INFO - with_bidirectional - Epoch 14/15, Loss: 0.4724, Accuracy: 0.9063\n",
      "2025-02-18 22:23:37,287 - INFO - with_bidirectional - Epoch 15/15, Loss: 0.4711, Accuracy: 0.9067\n",
      "2025-02-18 22:23:37,289 - INFO - \n",
      "Training full_model\n",
      "2025-02-18 22:24:43,407 - INFO - full_model - Epoch 1/15, Loss: 0.7006, Accuracy: 0.8546\n",
      "2025-02-18 22:25:49,539 - INFO - full_model - Epoch 2/15, Loss: 0.5911, Accuracy: 0.8748\n",
      "2025-02-18 22:26:55,605 - INFO - full_model - Epoch 3/15, Loss: 0.5530, Accuracy: 0.8861\n",
      "2025-02-18 22:28:01,550 - INFO - full_model - Epoch 4/15, Loss: 0.5309, Accuracy: 0.8877\n",
      "2025-02-18 22:29:09,423 - INFO - full_model - Epoch 5/15, Loss: 0.5191, Accuracy: 0.8956\n",
      "2025-02-18 22:30:16,464 - INFO - full_model - Epoch 6/15, Loss: 0.5088, Accuracy: 0.8971\n",
      "2025-02-18 22:31:25,255 - INFO - full_model - Epoch 7/15, Loss: 0.5002, Accuracy: 0.8991\n",
      "2025-02-18 22:32:36,269 - INFO - full_model - Epoch 8/15, Loss: 0.4924, Accuracy: 0.9015\n",
      "2025-02-18 22:33:45,466 - INFO - full_model - Epoch 9/15, Loss: 0.4888, Accuracy: 0.9015\n",
      "2025-02-18 22:35:02,836 - INFO - full_model - Epoch 10/15, Loss: 0.4824, Accuracy: 0.9033\n",
      "2025-02-18 22:36:18,152 - INFO - full_model - Epoch 11/15, Loss: 0.4794, Accuracy: 0.9068\n",
      "2025-02-18 22:37:33,974 - INFO - full_model - Epoch 12/15, Loss: 0.4751, Accuracy: 0.9081\n",
      "2025-02-18 22:38:46,154 - INFO - full_model - Epoch 13/15, Loss: 0.4730, Accuracy: 0.9092\n",
      "2025-02-18 22:40:02,442 - INFO - full_model - Epoch 14/15, Loss: 0.4700, Accuracy: 0.9092\n",
      "2025-02-18 22:41:17,872 - INFO - full_model - Epoch 15/15, Loss: 0.4692, Accuracy: 0.9090\n",
      "2025-02-18 22:41:17,874 - INFO - Results saved to ablation_results_20250218_224117.json\n",
      "2025-02-18 22:41:17,875 - INFO - \n",
      "Ablation Study Results:\n",
      "2025-02-18 22:41:17,876 - INFO - \n",
      "Model\t\tAccuracy\tF1\t\tAUC\t\tAP\n",
      "2025-02-18 22:41:17,877 - INFO - ----------------------------------------------------------------------\n",
      "2025-02-18 22:41:17,878 - INFO - base_model     0.9034\t0.8986\t0.9713\t0.9419\n",
      "2025-02-18 22:41:17,878 - INFO - with_attention 0.9070\t0.9022\t0.9721\t0.9438\n",
      "2025-02-18 22:41:17,879 - INFO - with_bidirectional0.9067\t0.9024\t0.9729\t0.9439\n",
      "2025-02-18 22:41:17,880 - INFO - full_model     0.9090\t0.9045\t0.9737\t0.9459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ablation study completed. Check ablation_study.log for detailed results.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class HierarchicalFederatedServer:\n",
    "    def __init__(self, global_model):\n",
    "        self.global_model = global_model\n",
    "        self.client_updates = []\n",
    "\n",
    "    def receive_update(self, update):\n",
    "        self.client_updates.append(update)\n",
    "\n",
    "    def aggregate_updates(self):\n",
    "        if not self.client_updates:\n",
    "            return\n",
    "        aggregated = copy.deepcopy(self.client_updates[0])\n",
    "        for key in aggregated.keys():\n",
    "            params = torch.stack([update[key] for update in self.client_updates])\n",
    "            aggregated[key] = torch.mean(params, dim=0)\n",
    "        self.global_model.load_state_dict(aggregated)\n",
    "        self.client_updates = []\n",
    "\n",
    "class FederatedClient:\n",
    "    def __init__(self, model, train_data, train_labels, device, learning_rate=0.0005):\n",
    "        self.model = copy.deepcopy(model).to(device)\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.02\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=learning_rate,\n",
    "            epochs=15,\n",
    "            steps_per_epoch=len(train_data) // 32 + 1\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    def train(self, epochs=15):\n",
    "        self.model.train()\n",
    "        dataset = DataLoader(\n",
    "            torch.utils.data.TensorDataset(self.train_data, self.train_labels),\n",
    "            batch_size=32,\n",
    "            shuffle=True\n",
    "        )\n",
    "        best_loss = float('inf')\n",
    "        patience = 3\n",
    "        no_improve = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for data, labels in dataset:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(data.float())\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            avg_loss = epoch_loss / len(dataset)\n",
    "            logging.info(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                \n",
    "            if no_improve >= patience:\n",
    "                logging.info(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "        return self.model.state_dict()\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.layer_norm1(x + attn_output)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm2(x + ff_output)\n",
    "        return x\n",
    "\n",
    "class ImprovedRNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_classes, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_dim, hidden_dim, num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            AttentionModule(hidden_dim * 2, dropout=dropout) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.feature_extractor(x).unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "\n",
    "        for attention_layer in self.attention_layers:\n",
    "            lstm_out = attention_layer(lstm_out)\n",
    "\n",
    "        pooled = F.adaptive_max_pool1d(lstm_out.transpose(1, 2), 1).squeeze(-1)\n",
    "        return self.classifier(pooled)\n",
    "    \n",
    "# Base model without attention and with simplified architecture\n",
    "class BaseRNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_classes, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_dim, hidden_dim, num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=False, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.feature_extractor(x).unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        pooled = lstm_out[:, -1, :]\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "# Model with attention but without bidirectional LSTM\n",
    "class RNNLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_classes, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_dim, hidden_dim, num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=False, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.feature_extractor(x).unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        \n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        lstm_out = self.layer_norm(lstm_out + attn_out)\n",
    "        pooled = F.adaptive_max_pool1d(lstm_out.transpose(1, 2), 1).squeeze(-1)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "# Model variants for ablation study\n",
    "def get_model_variants(input_size, hidden_dim, num_classes, train_data=None, train_labels=None, device=None):\n",
    "    # Basic model variants\n",
    "    base_variants = {\n",
    "        'base_model': BaseRNNLSTM(input_size, hidden_dim, num_classes),\n",
    "        'with_attention': RNNLSTMWithAttention(input_size, hidden_dim, num_classes),\n",
    "        'with_bidirectional': ImprovedRNNLSTM(input_size, hidden_dim, num_classes, num_layers=2, dropout=0.2),\n",
    "    }\n",
    "    \n",
    "    # Initialize full model\n",
    "    full_model = ImprovedRNNLSTM(input_size, hidden_dim, num_classes)\n",
    "    \n",
    "    # Add standard full model\n",
    "    variants = {\n",
    "        **base_variants,\n",
    "        'full_model': full_model,\n",
    "    }\n",
    "    \n",
    "    # Add federated learning variants if training data is provided\n",
    "    if train_data is not None and train_labels is not None and device is not None:\n",
    "        # Split data for federated learning\n",
    "        split_data = distribute_data(train_data, train_labels)\n",
    "        \n",
    "        # Federated Learning setup\n",
    "        fed_model = ImprovedRNNLSTM(input_size, hidden_dim, num_classes)\n",
    "        fed_clients = [\n",
    "            FederatedClient(fed_model, client_data, client_labels, device)\n",
    "            for client_data, client_labels in split_data\n",
    "        ]\n",
    "        variants['federated_model'] = {\n",
    "            'model': fed_model,\n",
    "            'clients': fed_clients\n",
    "        }\n",
    "        \n",
    "        # Hierarchical Federated Learning setup\n",
    "        hier_fed_model = ImprovedRNNLSTM(input_size, hidden_dim, num_classes)\n",
    "        hier_fed_server = HierarchicalFederatedServer(hier_fed_model)\n",
    "        hier_fed_clients = [\n",
    "            FederatedClient(hier_fed_model, client_data, client_labels, device)\n",
    "            for client_data, client_labels in split_data\n",
    "        ]\n",
    "        variants['hierarchical_federated_model'] = {\n",
    "            'model': hier_fed_model,\n",
    "            'server': hier_fed_server,\n",
    "            'clients': hier_fed_clients\n",
    "        }\n",
    "    \n",
    "    return variants\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, train_loader, test_loader, device):\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "\n",
    "    def train_standard_model(self, model, model_name, epochs=15):\n",
    "        \"\"\"Train a standard (non-federated) model\"\"\"\n",
    "        return self._train_single_model(model, model_name, epochs)\n",
    "\n",
    "    def train_federated_model(self, model_info, model_name, epochs=15):\n",
    "        \"\"\"Train a federated learning model\"\"\"\n",
    "        model = model_info['model']\n",
    "        clients = model_info['clients']\n",
    "        \n",
    "        # Train each client\n",
    "        for client_id, client in enumerate(clients):\n",
    "            logging.info(f\"{model_name} - Training client {client_id + 1}/{len(clients)}\")\n",
    "            client_state = client.train(epochs=epochs)\n",
    "            \n",
    "            # Aggregate client updates by averaging parameters\n",
    "            if client_id == 0:\n",
    "                aggregated_state = copy.deepcopy(client_state)\n",
    "            else:\n",
    "                for key in aggregated_state:\n",
    "                    aggregated_state[key] += client_state[key]\n",
    "        \n",
    "        # Average the parameters\n",
    "        for key in aggregated_state:\n",
    "            aggregated_state[key] /= len(clients)\n",
    "            \n",
    "        # Update global model\n",
    "        model.load_state_dict(aggregated_state)\n",
    "        return evaluate_model(model, self.test_loader, self.device)\n",
    "\n",
    "    def train_hierarchical_federated_model(self, model_info, model_name, epochs=15):\n",
    "        \"\"\"Train a hierarchical federated learning model\"\"\"\n",
    "        server = model_info['server']\n",
    "        clients = model_info['clients']\n",
    "        \n",
    "        # Train each client and send updates to server\n",
    "        for client_id, client in enumerate(clients):\n",
    "            logging.info(f\"{model_name} - Training client {client_id + 1}/{len(clients)}\")\n",
    "            client_state = client.train(epochs=epochs)\n",
    "            server.receive_update(client_state)\n",
    "        \n",
    "        # Aggregate updates at server\n",
    "        server.aggregate_updates()\n",
    "        return evaluate_model(server.global_model, self.test_loader, self.device)\n",
    "\n",
    "    def _train_single_model(self, model, model_name, epochs=15):\n",
    "        \"\"\"Helper method for training standard models\"\"\"\n",
    "        model = model.to(self.device)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=0.0005,\n",
    "            weight_decay=0.02\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=0.0005,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=len(self.train_loader)\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        best_metrics = None\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for data, labels in self.train_loader:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data.float())\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(self.train_loader)\n",
    "            metrics = evaluate_model(model, self.test_loader, self.device)\n",
    "            \n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_metrics = metrics\n",
    "            \n",
    "            logging.info(f'{model_name} - Epoch {epoch + 1}/{epochs}, '\n",
    "                        f'Loss: {avg_loss:.4f}, '\n",
    "                        f'Accuracy: {metrics[\"accuracy\"]:.4f}')\n",
    "        \n",
    "        return best_metrics\n",
    "\n",
    "# Updated AblationStudy class to handle federated variants\n",
    "class AblationStudy:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, device):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.device = device\n",
    "        self.results = defaultdict(dict)\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            torch.utils.data.TensorDataset(\n",
    "                torch.from_numpy(X_train), \n",
    "                torch.from_numpy(y_train)\n",
    "            ),\n",
    "            batch_size=32,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            torch.utils.data.TensorDataset(\n",
    "                torch.from_numpy(X_test), \n",
    "                torch.from_numpy(y_test)\n",
    "            ),\n",
    "            batch_size=64,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        self.trainer = ModelTrainer(self.train_loader, self.test_loader, device)\n",
    "\n",
    "    def run_ablation_study(self):\n",
    "        input_size = self.X_train.shape[2]\n",
    "        hidden_dim = 128\n",
    "        num_classes = len(np.unique(self.y_train))\n",
    "        \n",
    "        model_variants = get_model_variants(\n",
    "            input_size, hidden_dim, num_classes,\n",
    "            self.X_train, self.y_train, self.device\n",
    "        )\n",
    "        \n",
    "        for model_name, model_info in model_variants.items():\n",
    "            logging.info(f\"\\nTraining {model_name}\")\n",
    "            \n",
    "            if model_name in ['base_model', 'with_attention', 'with_bidirectional', 'full_model']:\n",
    "                metrics = self.trainer.train_standard_model(model_info, model_name)\n",
    "            elif model_name == 'federated_model':\n",
    "                metrics = self.trainer.train_federated_model(model_info, model_name)\n",
    "            elif model_name == 'hierarchical_federated_model':\n",
    "                metrics = self.trainer.train_hierarchical_federated_model(model_info, model_name)\n",
    "            \n",
    "            self.results[model_name] = metrics\n",
    "        \n",
    "        self.save_results()\n",
    "        return self.results\n",
    "\n",
    "class AblationStudy:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, device):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.device = device\n",
    "        self.results = defaultdict(dict)\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            torch.utils.data.TensorDataset(\n",
    "                torch.from_numpy(X_train), \n",
    "                torch.from_numpy(y_train)\n",
    "            ),\n",
    "            batch_size=32,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            torch.utils.data.TensorDataset(\n",
    "                torch.from_numpy(X_test), \n",
    "                torch.from_numpy(y_test)\n",
    "            ),\n",
    "            batch_size=64,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def train_model(self, model, model_name, epochs=15):\n",
    "        model = model.to(self.device)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=0.0005,\n",
    "            weight_decay=0.02\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=0.0005,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=len(self.train_loader)\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        best_metrics = None\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for data, labels in self.train_loader:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data.float())\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(self.train_loader)\n",
    "            metrics = evaluate_model(model, self.test_loader, self.device)\n",
    "            \n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_metrics = metrics\n",
    "            \n",
    "            logging.info(f'{model_name} - Epoch {epoch + 1}/{epochs}, '\n",
    "                        f'Loss: {avg_loss:.4f}, '\n",
    "                        f'Accuracy: {metrics[\"accuracy\"]:.4f}')\n",
    "        \n",
    "        return best_metrics\n",
    "\n",
    "    def run_ablation_study(self):\n",
    "        input_size = self.X_train.shape[2]\n",
    "        hidden_dim = 128\n",
    "        num_classes = len(np.unique(self.y_train))\n",
    "        \n",
    "        model_variants = get_model_variants(input_size, hidden_dim, num_classes)\n",
    "        \n",
    "        for model_name, model in model_variants.items():\n",
    "            logging.info(f\"\\nTraining {model_name}\")\n",
    "            metrics = self.train_model(model, model_name)\n",
    "            self.results[model_name] = metrics\n",
    "        \n",
    "        self.save_results()\n",
    "        return self.results\n",
    "\n",
    "    def save_results(self):\n",
    "        # Convert results to serializable format\n",
    "        results_dict = {\n",
    "            model_name: {\n",
    "                metric: float(value) \n",
    "                for metric, value in metrics.items()\n",
    "            }\n",
    "            for model_name, metrics in self.results.items()\n",
    "        }\n",
    "        \n",
    "        # Add timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'ablation_results_{timestamp}.json'\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results_dict, f, indent=4)\n",
    "        \n",
    "        logging.info(f\"Results saved to {filename}\")\n",
    "        \n",
    "        # Create comparison table in log\n",
    "        logging.info(\"\\nAblation Study Results:\")\n",
    "        logging.info(\"\\nModel\\t\\tAccuracy\\tF1\\t\\tAUC\\t\\tAP\")\n",
    "        logging.info(\"-\" * 70)\n",
    "        \n",
    "        for model_name, metrics in self.results.items():\n",
    "            logging.info(\n",
    "                f\"{model_name:<15}\"\n",
    "                f\"{metrics['accuracy']:.4f}\\t\"\n",
    "                f\"{metrics['f1']:.4f}\\t\"\n",
    "                f\"{metrics['auc']:.4f}\\t\"\n",
    "                f\"{metrics['ap']:.4f}\"\n",
    "            )\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    data = pd.read_csv('data/S-FFSD.csv', encoding='utf-8')\n",
    "    \n",
    "    # Feature engineering\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['Time'] % 24 / 24)\n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['Time'] % 24 / 24)\n",
    "    data['amount_log'] = np.log1p(data['Amount'])\n",
    "    \n",
    "    # Define feature columns\n",
    "    categorical_cols = ['Source', 'Target', 'Location', 'Type']\n",
    "    numeric_cols = ['Amount', 'Time', 'amount_log', 'hour_sin', 'hour_cos']\n",
    "\n",
    "    # Process categorical columns\n",
    "    for col in categorical_cols:\n",
    "        data[f'{col}_freq'] = data[col].map(data[col].value_counts(normalize=True))\n",
    "        data[col] = LabelEncoder().fit_transform(data[col])\n",
    "\n",
    "    # Combine all features\n",
    "    feature_cols = numeric_cols + categorical_cols + [f'{col}_freq' for col in categorical_cols]\n",
    "    X = data[feature_cols].values\n",
    "    y = data['Labels'].values\n",
    "\n",
    "    # Scale features\n",
    "    scaler = RobustScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = np.clip(X, -5, 5)  # Clip extreme values\n",
    "    X = X.reshape(-1, 1, X.shape[1]).astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0)  # Handle any NaN values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data.float())\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Metrics computation\n",
    "    accuracy = (all_preds == all_labels).mean()\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
    "    \n",
    "    # Convert labels to one-hot encoding for average precision score\n",
    "    all_labels_onehot = label_binarize(all_labels, classes=np.unique(all_labels))\n",
    "    ap = average_precision_score(all_labels_onehot, all_probs, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'ap': ap\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def run_ablation_experiment():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y = load_and_preprocess_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize and run ablation study\n",
    "    study = AblationStudy(X_train, y_train, X_test, y_test, device)\n",
    "    results = study.run_ablation_study()\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('ablation_study.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    results = run_ablation_experiment()\n",
    "    print(\"\\nAblation study completed. Check ablation_study.log for detailed results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
